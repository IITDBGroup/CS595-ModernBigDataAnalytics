---
shortname: distributed_file_systems
title: Storage - Distributed File Systems
---

<section data-markdown data-background="{{ "/assets/images/dbgrouplogo.png" | prepend: site.baseurl }}" data-background-size="250px" data-background-position="right 5% bottom 15%">
	<textarea data-template>
# {{ site.data.course.title }} - {{ page.title }}

**Lecturer**: [Boris Glavic]({{ site.data.instructor.home }})

**Semester**: {{ site.data.course.semester }} {{ site.data.course.year }}

	</textarea>
</section>

<section data-markdown>
  <textarea data-template>
# 2. Distributed Storage

## Distributed File Systems
  </textarea>
</section>

<section data-markdown>
  <textarea data-template>
## Distributed File System

- Store files in a cluster of machines
- Manage **meta-data** (file-system structure, permissions, ...) and **data** (file content)
- **Operations:**
  - Create / delete files
  - Directory operations
  - Read file (sequentially / random access)
  - Write file (append / random access)
  </textarea>
</section>

<section data-markdown>
  <textarea data-template>
## Requirements

- Support files larger than storage of a single machine
- **Fault Tolerance**
   - **Data loss**: Do not loose data when a node in the cluster fails
   - **Availability**: data should be accessible even under
      - network failures
      - node failures
  </textarea>
</section>

<section data-markdown>
  <textarea data-template>
## Requirements cont.

- **Load balancing**
  - Distribute file system operations across the cluster
  - Balance operations across the cluster
- **Transparency**
  - clients do not need to decide on data distribution
  - clients do not need to handle fault tolerance
  </textarea>
</section>


<section data-markdown>
  <textarea data-template>
## Fault tolerance - Data loss

- If data is stored on only one node, then data loss cannot be prevented
- => Each piece of data has to be stored on multiple nodes (**replication**) or at least some additional information has to be stored on other nodes to enable recovery of lost data (e.g., erasure coding)
  </textarea>
</section>

<section data-markdown>
  <textarea data-template>
## Replication

- Each piece of data is replicated across $m$ nodes
- How to choose number of replicas?
- How to keep replicas in sync (consistency)?
- How to detect missing replicas and compensate for that?
  </textarea>
</section>

<section data-markdown>
  <textarea data-template>
## How to chooose the number of replicas?

- Larger $m$
    - Wasting storage
    - Decreased chance of data loss
- Smaller $m$
    - Less wasted storage
    - Higher chance of data loss
- Sample data point:
  - 2-3-way replication sufficient for 99.9% reliability
  </textarea>
</section>

<section data-markdown>
  <textarea data-template>
## Characteristics of Replication

- **Read performance**
    - If all replicas are kept up to sync, then we can read from multiple replicas in parallel
    - $m$-way replication improves read through-put by a factor of $m$
  </textarea>
</section>



<section data-markdown>
  <textarea data-template>
## Characteristics of Replication cont.

- **Write performance**
    - Have to write to all replicas
    - In addition to syncronization overhead (consistency) this causes $m$-times the write load
- **Storage requirements**
    - increased by a factor of $m$
  </textarea>
</section>

<section data-markdown>
  <textarea data-template>
## Fault tolerance - Network issues

- Replication can help too
- Need to be aware of network infrastructure
  - Do not place all replicas on nodes that are connected to the same switch
  </textarea>
</section>

<section data-markdown>
  <textarea data-template>
## Data Placement

- How to balance data distribution across a cluster?
- At a file level?
  - High computational complexity (NP-hard)
- Split files into blocks
  - Distribute individual blocks
  - What is a good block size?
  </textarea>
</section>

<section data-markdown>
  <textarea data-template>
## Data Placement cont.

- How to determine which block goes where?
- e.g., hash function
  - if number of blocks is large enough that almost guarantees even distribution for the right choice of hash functions
  </textarea>
</section>


<section data-markdown>
  <textarea data-template>
## Meta-data Management

- **Dedicated nodes for meta-data management**
  - A subsets of $m < n$ nodes in the cluster handle metadata management
- **Truely distributed meta-data management**
  - All nodes participate in meta-data management
  </textarea>
</section>

<section data-markdown>
  <textarea data-template>
## Consistency

- Allow full parallel access to files for random reads and writes
    - Readers and writers need to synchronize for all operations
      - or a weaker consistency model, e.g., eventual consistency, has to be applied
    - Essentially same problems as in transaction processing
        - => Flexible, but requires complicated and expensive strong consistency
  </textarea>
</section>

<section data-markdown>
  <textarea data-template>
## Consistency

- Limit file operations and/or restricting concurrent access
    - e.g., append-only or no modification after creation
    - only one writer per file at a time
  </textarea>
</section>




<!-- ******************************************************************************** -->
<!-- HADOOP -->
<!-- ******************************************************************************** -->
<section data-markdown>
  <textarea data-template>
# 2. Distributed Storage

## Hadoop Distributed Filesystem (HDFS)
  </textarea>
</section>

<section data-markdown>
  <textarea data-template>
## HDFS

- Open-source distributed file system
- Modelled after Google's Distributed Filesystem (GFS)
- Written in Java
- Optimized for storage of large files
  </textarea>
</section>

<section data-markdown>
  <textarea data-template>
## File System Structure

- An HDFS file system is made of **inodes** (directories or files) which have associated **metadata** (e.g., permissions)
- Files consist of one or more **blocks**
    - The block size is much larger than on single node file systems (e.g., 128MB)
- Some blocks may be smaller than the block size
    - *200MB file: 128MB + 72MB block*
    - *4KB file: 4KB block*
  </textarea>
</section>


<section data-markdown>
  <textarea data-template>
## Architecture

- **Name node**
  - node in the cluster responsible for storing filesystem meta-data
    - directory structure
    - inode metadata (permissions, ...)
    - which blocks belong to which files
  - handles client requests for FS metadata
  - single name node per cluster (possible hot or cold stand-bys)
  </textarea>
</section>
<section data-markdown>
  <textarea data-template>
## Architecture

- **Data node**
    - stores file content (blocks and block metadata)
    - clients directly communicating with data nodes for reading/writing
    - All nodes in the cluster except for name nodes (and potentially some other exceptions discussed later) are data nodes
  </textarea>
</section>

<section>
{% graphviz %}
digraph G {

node[shape=rectangle,style="filled,rounded", fillcolor="lightcyan", color=black];
rankdir="TB";
newrank=true;

   subgraph cluster_0 {
        rank=same;
		style="filled,rounded";
        fillcolor=wheat1;
		node [shape=rectangle,style="filled,rounded",fillcolor="darkolivegreen2",color=black];
        f2[label="File2: B3"];
        f1[label="File1: B1, B2"];
        fontname="times-bold";
		label = "Name Node";
	}

   subgraph cluster_1 {
        rank=same;
		style="filled,rounded";
        fillcolor=wheat1;
		node [shape=rectangle,style="filled,rounded",fillcolor="darkolivegreen2",color=black];
        b12[label="Block 2"];
        b11[label="Block 1"];
        fontname="times-bold";
		label = "Data Node";
	}

  subgraph cluster_2 {
        rank=same;
		style="filled,rounded";
        fillcolor=wheat1;
		node [shape=rectangle,style="filled,rounded",fillcolor="darkolivegreen2",color=black];
        b23[label="Block 3"];
        b21[label="Block 1"];
        fontname="times-bold";
		label = "Data Node";
	}

  subgraph cluster_3 {
        rank=same;
		style="filled,rounded";
        fillcolor=wheat1;
		node [shape=rectangle,style="filled,rounded",fillcolor="darkolivegreen2",color=black];
        b33[label="Block 3"];
        b32[label="Block 2"];
        fontname="times-bold";
		label = "Data Node";
	}

    f1 -> b11 [style=invis];
    b21 -> b32 [style=invis];

}
{% endgraphviz %}
</section>

<section data-markdown>
  <textarea data-template>
## Fault tolerance - data loss

- Each block is replicated to multiple data nodes (2-3 is typical)
- Name node tracks which data nodes store which blocks
- If a replica of block `b` is lost (e.g., node failure) then the name node instructs a data node storing  `b` to send the block to another data node to restore the desired number of replicas
  </textarea>
</section>

<section data-markdown>
<textarea data-template>
## Example - Restoring Replication Count

- For this example assume that data node 3 has failed and we are using 2-way replication
- blocks 2 and 3 have to be replicated once more to restore that there exist 2 replicas for each
</textarea>
</section>

<section>
{% graphviz %}
digraph G {

node[shape=rectangle,style="filled,rounded", fillcolor="lightcyan", color=black];
rankdir="TB";
newrank=true;

   subgraph cluster_0 {
        rank=same;
		style="filled,rounded";
        fillcolor=wheat1;
		node [shape=rectangle,style="filled,rounded",fillcolor="darkolivegreen2",color=black];
        f2[label="File2: B3"];
        f1[label="File1: B1, B2"];
        fontname="times-bold";
		label = "Name Node";
	}


  subgraph cluster_3 {
        rank=same;
		style="filled,rounded";
        fillcolor="firebrick1";
		node [shape=rectangle,style="filled,rounded",fillcolor="darkolivegreen2",color=black];
        b33[label="Block 3"];
        b32[label="Block 2"];
        fontname="times-bold";
		label = "Data Node 3 (Failed)";
	}

   subgraph cluster_1 {
        rank=same;
		style="filled,rounded";
        fillcolor=wheat1;
		node [shape=rectangle,style="filled,rounded",fillcolor="darkolivegreen2",color=black];
        b12[label="Block 2"];
        b11[label="Block 1"];
        fontname="times-bold";
		label = "Data Node 1";
	}

  subgraph cluster_2 {
        rank=same;
		style="filled,rounded";
        fillcolor=wheat1;
		node [shape=rectangle,style="filled,rounded",fillcolor="darkolivegreen2",color=black];
        b23[label="Block 3"];
        b21[label="Block 1"];
        fontname="times-bold";
		label = "Data Node 2";
	}


    f2 -> b12 [style="invis"];
    f2 -> b21  [style="invis"];
    f2->b32 [style="invis"];
    b12-> b21 [ label="transfer"];
    b23-> b11 [ label="transfer"];

}
{% endgraphviz %}
</section>



<section data-markdown>
  <textarea data-template>
## Fault tolerance - node failure

- data nodes send **heart beat** messages to name node every 3 seconds
  - piggyback storage utilization and load stats
- If the name nodes has not received a heart beat from a data node in a certain amount of time, it schedules the creation of new replicas of the blocks stored on this node
  </textarea>
</section>

<section>
{% graphviz %}
digraph G {

node [shape=rectangle,style="filled,rounded",fillcolor="darkolivegreen2",color=black,fontname="times-bold"];
rankdir="LR";
newrank=true;

nn [label="Name Node"];
d1 [label="Data Node 1"];
d2 [label="Data Node 2"];
d3 [label="Data Node 3"];

d1 -> nn [label="Heartbeat"];
d2 -> nn [label="Heartbeat"];
d3 -> nn [label="Heartbeat"];
}
{% endgraphviz %}
</section>

<section data-markdown>
  <textarea data-template>
## Fault tolerance - network failures

- HDFS's block replica placement strategy is network topology aware
- Replicas of blocks are stored on separate racks to avoid loss of availability when a switch connecting the nodes in a rack to the cluster is down
  </textarea>
</section>

<section>
{% graphviz %}
digraph G {

node [shape=rectangle,style="filled,rounded",fillcolor="darkolivegreen2",color=black,fontname="times-bold"];
rankdir="TB";

i[label="interconnect"];


node[shape=rectangle,style="filled,rounded", fillcolor="lightcyan", color=black];

rank=same;

   subgraph cluster_0 {
       rankdir="LR";
		style="filled,rounded";
        fillcolor=wheat1;
        s1[label="Switch"];
        n1[label="Node 1",fillcolor="darkolivegreen2"];
        n2[label="Node 2",fillcolor="darkolivegreen2"];
        s1->n1;
        s1->n2;
        fontname="times-bold";
		label = "Rack 1";
	}


   subgraph cluster_1 {
       rankdir="LR";
		style="filled,rounded";
        fillcolor=wheat1;
        s2[label="Switch"];
        n3[label="Node 3",fillcolor="darkolivegreen2"];
        n4[label="Node 4",fillcolor="darkolivegreen2"];
        s2->n3;
        s2->n4;
        fontname="times-bold";
		label = "Rack 2";
	}

    i->s1;
    i->s2;
}
{% endgraphviz %}
</section>


<section data-markdown>
  <textarea data-template>
## Fault tolerance - data corruption

- when a client writes or reads a block, it computes a **checksum**
- data nodes store the checksums for blocks and send them to clients reading the block
- if the checksum computed based on a read block is different from the stored checksum, then this replica is dropped
  </textarea>
</section>



<section data-markdown>
  <textarea data-template>
## Reading

- To read a file, the client sends a request to the name node
- the name node sends the client
  - the list of blocks for the file
  - the location of all replicas for each block
- the client then contacts data nodes directly to read blocks
  </textarea>
</section>

<section>
{% graphviz %}
digraph G {

node[shape=rectangle,style="filled,rounded", fillcolor="lightcyan", color=black];
rankdir="TB";
newrank=true;

   subgraph cluster_1 {
        rank=same;
		style="filled,rounded";
        fillcolor=wheat1;
		node [shape=rectangle,style="filled,rounded",fillcolor="darkolivegreen2",color=black];
        b12[label="Block 2"];
        b11[label="Block 1"];
        fontname="times-bold";
		label = "Data Node 1";
	}

   subgraph cluster_0 {
        rank=same;
		style="filled,rounded";
        fillcolor=wheat1;
		node [shape=rectangle,style="filled,rounded",fillcolor="darkolivegreen2",color=black];
        f2[label="File2: B3"];
        f1[label="File1: B1, B2"];
        fontname="times-bold";
		label = "Name Node";
	}


  subgraph cluster_2 {
        rank=same;
		style="filled,rounded";
        fillcolor=wheat1;
		node [shape=rectangle,style="filled,rounded",fillcolor="darkolivegreen2",color=black];
        b23[label="Block 3"];
        b21[label="Block 1"];
        fontname="times-bold";
		label = "Data Node 2";
	}


    client[label="Client",fontname="times-bold"];


    client -> f1;
    f1->client [label="metadata"];
    client->b12 [label="read"];
    client->b21 [label="read"];
    b11->f1[style="invis"];

}
{% endgraphviz %}
</section>


<section data-markdown>
<textarea data-template>
## Writing & Consistency

- Writing is append only in HDFS
- At any time, only one client can write a file
- The name node maintains locks for each file and declines write requests by clients that want to write a file that is currently locked
</textarea>
</section>

<section data-markdown>
<textarea data-template>
## Writing a Block

- A client holding a lock for a file contacts the name node for each block it is writing to get the list of data nodes that will store replicas of the block
- A pipeline is established between the client and the data nodes that will store replicas
- Data is send in smaller chunks
  - from client to first data node
  - from first data node to second data node
  - ...
</textarea>
</section>

<section>
<div class="scrollable-slide">
{% seqdiag size="50x300" %}
seqdiag {
Client => "Data Node 1" => "Data Node 2" [label = "setup", return =""];
Client => "Data Node 1" => "Data Node 2" [label = "block 1", return =""];
Client => "Data Node 1" => "Data Node 2" [label = "...", return =""];
Client => "Data Node 1" => "Data Node 2" [label = "block n", return =""];
Client => "Data Node 1" => "Data Node 2" [label = "close", return =""];
}
{% endseqdiag %}
</div>
</section>

<section data-markdown>
<textarea data-template>
## Concurrent Writing and Reading

- blocks written to a file are hidden from readers until either
  - the client closes the file
  - the client issues a flush operation
- in either case the name nodes updates the file metadata to include the new blocks
</textarea>
</section>

<section data-markdown>
<textarea data-template>
## Metadata operations

- the name node keeps the file system metadata in memory
- periodically a snapshot is written to disk
- changes to metadata are ...
  - not directly applied to the in memory copy of the metadata
  - written to journal (WAL)
- taking a snapshot
  - apply journal to copy of metadata and write to disk
</textarea>
</section>

<section data-markdown>
<textarea data-template>
## Name Node Stand-by

- WAL enables to keep a stand-by name node
  - recover from name node failures
- The name node sends journal entries to the stand-by which applies the journal to its memory snapshot
- Can also be used to reduce load on the name node by outsourcing of taking snapshots
</textarea>
</section>

<section data-markdown>
<textarea data-template>
## HDFS Summary

- **fault tolerance**: replication
   - detection of failures through heart beat
   - replica placement is topology-aware
- **metadata management**:
  - single node (name node)
- **consistency**:
  - prevent concurrent writes
  - changes to file are atomic (from a reader's perspective)
</textarea>
</section>

<section data-markdown>
<textarea data-template>
## Advantages & Disadvantages

- **Advantages**
  - well suited for batch processing
  - decent read performance
  - fault tolerant
</textarea>
</section>

<section data-markdown>
<textarea data-template>
## Advantages & Disadvantages

- **Disadvantages**
  - Name node is a bottle-neck for metadata operations
  - Inflexible write operations
  - relatively poor write performance
  - No concurrent writes
  - No semantic data placement (more on this later)
</textarea>
</section>

<!-- Local Variables: -->
<!-- mode: markdown -->
<!-- End: -->
